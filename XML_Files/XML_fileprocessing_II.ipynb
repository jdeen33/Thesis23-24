{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuwSNYrNz1Pr"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvJmxCi7R54W"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup, Tag\n",
        "import requests\n",
        "import pprint\n",
        "!pip install stanza\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UstXlp6d879y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgz-rHhSALb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fad34f8-8b64-4ab5-a644-6caaa21420ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAEyToy5AeJd"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/Thesis2023/philoalex-main.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksTk0tQcBnFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66e9fdb-ddd7-40ea-f6d1-b8b2cecc43a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/tlg0018.tlg002.opp-grc1.xml',\n",
              " '/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/tlg0018.tlg019.opp-grc1.xml',\n",
              " '/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/tlg0018.tlg022.opp-grc1.xml',\n",
              " '/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/tlg0018.tlg024.opp-grc1.xml']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "grc_files = sorted(glob.glob('/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/*.opp-grc1.xml'))\n",
        "grc_files\n",
        "#grc_colson="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-JLV-DvIms_"
      },
      "outputs": [],
      "source": [
        "#use whichever language you need\n",
        "#nlp_grc = stanza.Pipeline(lang='grc', processors='tokenize')\n",
        "nlp = stanza.Pipeline(lang='grc', processors='tokenize')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "philo = open( '/content/corr-tlg0018.tlg020.opp-grc1.xml' ,encoding='utf-8', errors='ignore')\n",
        "philo_soup = BeautifulSoup(philo,'lxml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_h6k51ACpp1",
        "outputId": "79267f14-2f7b-4525-97a8-e174e3101d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "philo_soup"
      ],
      "metadata": {
        "id": "xiEUaMKPCvEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections= philo_soup.find_all('div', subtype= 'section')\n",
        "for sec in sections:\n",
        "  sec['id']= 'Section',sec['n']\n"
      ],
      "metadata": {
        "id": "OyjsUSYkDQjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notes= philo_soup.find_all(\"note\")\n",
        "for note in notes:\n",
        "  note.decompose()\n",
        "    #removing marked mistakes from Prof Crane's corrected file (<sic> tag)\n",
        "sic= philo_soup.find_all('sic')\n",
        "for s in sic:\n",
        "  s.decompose()\n",
        "    #removing page break tags\n",
        "pages = philo_soup.find_all('pb')\n",
        "for p in pages:\n",
        "  p.decompose()\n",
        "    #removing line break tags\n",
        "breaks = philo_soup.find_all('lb')\n",
        "for b in breaks:\n",
        "  b.decompose()\n"
      ],
      "metadata": {
        "id": "tPBo72FEHkyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sec in philo_soup.find_all('div', subtype= 'section'):\n",
        "  for p in sec.find_all('p'):\n",
        "    p['id']= sec['id']"
      ],
      "metadata": {
        "id": "YwUbI7kKHkop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "philo_soup"
      ],
      "metadata": {
        "id": "RSXBM4DcIMqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paras = philo_soup.find_all('p')\n",
        "for para in paras:\n",
        "  doc= nlp(para.get_text())   #applies tokenizer\n",
        "  sents= [sentence.text for sentence in doc.sentences]  #acceses tuple that stanza makes of sentences\n",
        "  para.clear()  #not sure why this needs to be in here to work but...it does\n",
        "  for sent_index, sent in enumerate(sents): #enumersates the sentences for each section\n",
        "    sent_tag = philo_soup.new_tag('s',n= sent_index+1)\n",
        "    sent_tag.string = sent #identifies contents of the tag\n",
        "    sec.append(sent_tag) #appends contents to the correct tag\n",
        "\n"
      ],
      "metadata": {
        "id": "v94qSYiyDbMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx9aw8u4z5Jj"
      },
      "source": [
        "XML Edits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg2dqrK_R_QW"
      },
      "outputs": [],
      "source": [
        "#loading the file with beautiful soup\n",
        "def open_file(x):\n",
        "  philo = open( x ,encoding='utf-8', errors='ignore')\n",
        "  philo_soup = BeautifulSoup(philo,'lxml')\n",
        "  return philo_soup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwWwr6vV-5bB"
      },
      "outputs": [],
      "source": [
        "#removing extra tags that we don't need for text tokenizing\n",
        "def removing_tags(philo_soup):\n",
        "  notes= philo_soup.find_all(\"note\")\n",
        "  for note in notes:\n",
        "    note.decompose()\n",
        "    #removing marked mistakes from Prof Crane's corrected file (<sic> tag)\n",
        "  sic= philo_soup.find_all('sic')\n",
        "  for s in sic:\n",
        "    s.decompose()\n",
        "    #removing page break tags\n",
        "  pages = philo_soup.find_all('pb')\n",
        "  for p in pages:\n",
        "    p.decompose()\n",
        "    #removing line break tags\n",
        "  breaks = philo_soup.find_all('lb')\n",
        "  for b in breaks:\n",
        "    b.decompose()\n",
        "  return philo_soup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#alternate\n",
        "def making_books_and_secs_II(philo_soup):\n",
        "  for b in philo_soup.find_all('div', subtype= 'book'):\n",
        "    b['id']= 'Book'+(str(b['n']))\n",
        "    for sec in b.find_all('div', subtype= 'section'):\n",
        "      sec['id']= b['id']+ 'Section'+(str(sec['n']))\n",
        "      for para in sec.find_all('p'):\n",
        "        para['id']= sec['id']\n",
        "  return philo_soup"
      ],
      "metadata": {
        "id": "YZgQcoQZKkkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alternate\n",
        "#not all indexes are aligned correctly with the section number\n",
        "def make_sections(philo_soup):\n",
        "  sections= philo_soup.find_all('div',subtype= 'section')\n",
        "  paras= philo_soup.find_all('p')\n",
        "  for sec in sections:\n",
        "    sec['id']= 'Section'+ (str(sec['n']))\n",
        "    for para in paras:\n",
        "      para['id']= sec['id']\n",
        "  return philo_soup\n"
      ],
      "metadata": {
        "id": "mXzxFWAEKGVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alternate\n",
        "def sentence_tokenization(philo_soup):\n",
        "  sections= philo_soup.find_all('div',subtype= 'section')\n",
        "  for sec in sections:\n",
        "    doc= nlp(sec.get_text())   #applies tokenizer\n",
        "    sents= [sentence.text for sentence in doc.sentences]  #acceses tuple that stanza makes of sentences\n",
        "    sec.clear()  #not sure why this needs to be in here to work but...it does\n",
        "    for sent_index, sent in enumerate(sents): #enumersates the sentences for each section\n",
        "      sent_tag = philo_soup.new_tag('s', n= sent_index+1)\n",
        "      sent_tag.string = sent #identifies contents of the tag\n",
        "      sec.append(sent_tag) #appends contents to the correct tag\n",
        "  return philo_soup\n"
      ],
      "metadata": {
        "id": "Lj65D1BHLyZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alternate\n",
        "def making_sent_tags_II(philo_soup):\n",
        "  for sec in philo_soup.find_all('div',subtype= 'section'):\n",
        "    for s in sec.find_all('s'):\n",
        "      s['id']= sec['id']+'Sent'+(str(s['n']))\n",
        "  return philo_soup"
      ],
      "metadata": {
        "id": "b4Ffg8WON7c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def books_and_sections(philo_soup):\n",
        "  books= philo_soup.find_all('div', subtype= 'book')\n",
        "  for b_index, book in enumerate(books):\n",
        "    book['id']= f'Book{b_index}'\n",
        "    sections= philo_soup.find_all('p')\n",
        "    for sec_index, sec in enumerate(sections):\n",
        "      doc= nlp(sec.get_text())   #applies tokenizer\n",
        "      sents= [sentence.text for sentence in doc.sentences]  #acceses tuple that stanza makes of sentences\n",
        "      sec.clear()  #not sure why this needs to be in here to work but...it does\n",
        "      for sent_index, sent in enumerate(sents): #enumersates the sentences for each section\n",
        "        sent_tag = philo_soup.new_tag('s',id= f'Book{b_index-1}section{sec_index}sent{sent_index+1}')\n",
        "        sent_tag.string = sent #identifies contents of the tag\n",
        "        sec.append(sent_tag) #appends contents to the correct tag\n",
        "  return philo_soup\n"
      ],
      "metadata": {
        "id": "2hoF5mQpnugu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyRROYRGc7LO"
      },
      "outputs": [],
      "source": [
        "#numbers sections based on index\n",
        "#works only if sections numbers and index are aligned\n",
        "#otherwise use s[id]= section, p['n]\n",
        "def making_sections(philo_soup):\n",
        "  sections= philo_soup.find_all('p')\n",
        "  for index, s in enumerate(sections):\n",
        "    s['id']= f'Section{index}'\n",
        "\n",
        "  return philo_soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIQrdBudXqs7"
      },
      "outputs": [],
      "source": [
        "def making_sent_tags(philo_soup):\n",
        "  sections= philo_soup.find_all('p')\n",
        "  for sec_index, sec in enumerate(sections):\n",
        "    doc= nlp(sec.get_text())   #applies tokenizer\n",
        "    sents= [sentence.text for sentence in doc.sentences]  #acceses tuple that stanza makes of sentences\n",
        "    sec.clear()  #not sure why this needs to be in here to work but...it does\n",
        "    for sent_index, sent in enumerate(sents): #enumersates the sentences for each section\n",
        "      sent_tag = philo_soup.new_tag('s',id= f'section{sec_index}sent{sent_index+1}')\n",
        "      sent_tag.string = sent #identifies contents of the tag\n",
        "      sec.append(sent_tag) #appends contents to the correct tag\n",
        "  return philo_soup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aAhxIzDHi2k"
      },
      "outputs": [],
      "source": [
        "def write_file(philo_soup,x):\n",
        "  with open(f\"{x}_edited\", 'w') as philo_file:\n",
        "    philo_file.write(str(philo_soup))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I17YCQjH6T2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468efed2-c312-4cbd-e909-7741d1b4b1d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "for x in grc_files:\n",
        "  opened= open_file(x)\n",
        "  removed = removing_tags(opened)\n",
        "  sected= making_books_and_secs_II(removed)\n",
        "  tokenized= sentence_tokenization(sected)\n",
        "  sented= making_sent_tags_II(tokenized)\n",
        "  written= write_file(sented,x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD3xRIS8jJB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06189f6-39af-4af9-f9bc-60fa60365ba1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/tlg0018.tlg002.opp-grc1.xml_edited',\n",
              " '/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/tlg0018.tlg019.opp-grc1.xml_edited',\n",
              " '/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/tlg0018.tlg022.opp-grc1.xml_edited',\n",
              " '/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/tlg0018.tlg024.opp-grc1.xml_edited']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "corr_files = sorted(glob.glob('/content/drive/MyDrive/Thesis2023/XML Files/Yonge_Books/*-grc1.xml_edited'))\n",
        "corr_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lSLSTMNu1VZ"
      },
      "outputs": [],
      "source": [
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP9fdWrzvaz8"
      },
      "outputs": [],
      "source": [
        "\n",
        "with zipfile.ZipFile('cohn_corr__books_grc_.zip', 'w') as f:\n",
        "    for y in corr_files:\n",
        "        f.write(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJH8rz3a8twD"
      },
      "outputs": [],
      "source": [
        "#used this workflow/ code for the ones with books and sections\n",
        "#not the most efficient but moves down the tree successively adding more info\n",
        "for sec in philo_soup.find_all(subtype= 'section'):\n",
        "  for para in sec.find_all('p'):\n",
        "    para['id']= sec['id']\n",
        "    for sent in para.find_all('s'):\n",
        "      sent['id']= para['id']+'sent'+(str(sent['n']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}